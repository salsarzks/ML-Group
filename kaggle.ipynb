{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87b01537",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import basic packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55b53511",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_29668\\2337247494.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m## importing for model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mBidirectional\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mGRU\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "## importing for model\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding,Bidirectional,LSTM,GRU,Dense\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.models import load_model\n",
    "import urllib.request\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27687637",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import NLTK\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ed9127",
   "metadata": {},
   "outputs": [],
   "source": [
    "## to ignore Warnings\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e92a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## WIKI English Vectors file for images\n",
    "\n",
    "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562c901b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## unzip\n",
    "\n",
    "!unzip wiki-news-300d-1M.vec.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c4f744",
   "metadata": {},
   "outputs": [],
   "source": [
    "## lets load train data\n",
    "\n",
    "f=open('/kaggle/input/textemotiondetection/train.txt','r')\n",
    "x_train=[]\n",
    "y_train=[]\n",
    "for i in f:\n",
    "    l=i.split(';')\n",
    "    y_train.append(l[1].strip())\n",
    "    x_train.append(l[0])\n",
    "## lets load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ef3b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "## lets load test data\n",
    "\n",
    "f=open('/kaggle/input/textemotiondetection/test.txt','r')\n",
    "x_test=[]\n",
    "y_test=[]\n",
    "for i in f:\n",
    "    l=i.split(';')\n",
    "    y_test.append(l[1].strip())\n",
    "    x_test.append(l[0])\n",
    "\n",
    "f=open('/kaggle/input/textemotiondetection/val.txt','r')\n",
    "for i in f:\n",
    "    l=i.split(';')\n",
    "    y_train.append(l[1].strip())\n",
    "    x_train.append(l[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaab52e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## lets convert dataframe\n",
    "\n",
    "data_train=pd.DataFrame({'Text':x_train,'Emotion':y_train})\n",
    "data_test=pd.DataFrame({'Text':x_test,'Emotion':y_test})\n",
    "data=data_train.append(data_test,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9036f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## helper function to clean data\n",
    "\n",
    "def clean_text(data):\n",
    "    data=re.sub(r\"(#[\\d\\w\\.]+)\", '', data)\n",
    "    data=re.sub(r\"(@[\\d\\w\\.]+)\", '', data)\n",
    "    data=word_tokenize(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b81f328",
   "metadata": {},
   "outputs": [],
   "source": [
    "## lets apply\n",
    "\n",
    "texts=[' '.join(clean_text(text)) for text in data.Text]\n",
    "texts_train=[' '.join(clean_text(text)) for text in x_train]\n",
    "texts_test=[' '.join(clean_text(text)) for text in x_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11010a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenixe the data\n",
    "\n",
    "tokenizer=Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequence_train=tokenizer.texts_to_sequences(texts_train)\n",
    "sequence_test=tokenizer.texts_to_sequences(texts_test)\n",
    "index_of_words=tokenizer.word_index\n",
    "vocab_size=len(index_of_words)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbceafbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## lets inilize the model\n",
    "\n",
    "num_classes=6\n",
    "embed_num_dims=300\n",
    "max_seq_len=500\n",
    "class_names=['anger','sadness','fear','joy','surprise','love']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0214a6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## classes\n",
    "\n",
    "X_train_pad=pad_sequences(sequence_train,maxlen=max_seq_len)\n",
    "X_test_pad=pad_sequences(sequence_test,maxlen=max_seq_len)\n",
    "encoding={'anger':0,'sadness':1,'fear':2,'joy':3,'surprise':4,'love':5}\n",
    "y_train=[encoding[x] for x in data_train.Emotion]\n",
    "y_test=[encoding[x] for x in data_test.Emotion]\n",
    "y_train=to_categorical(y_train)\n",
    "y_test=to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90be6ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## word embedding function\n",
    "\n",
    "def create_embedding_matrix(filepath,word_index,embedding_dim):\n",
    "    vocab_size=len(word_index)+1\n",
    "    embedding_matrix=np.zeros((vocab_size,embedding_dim))\n",
    "    with open(filepath) as f:\n",
    "        for line in f:\n",
    "            word,*vector=line.split()\n",
    "            if word in word_index:\n",
    "                idx=word_index[word]\n",
    "                embedding_matrix[idx] = np.array(vector,dtype=n\n",
    "p.float32)[:embedding_dim]\n",
    "return embedding_matrix\n",
    "fname='wiki-news-300d-1M.vec'\n",
    "embedd_matrix=create_embedding_matrix(fname,index_of_words,embed_num_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34388f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## lets build model\n",
    "\n",
    "embedd_layer=Embedding(vocab_size,embed_num_dims,input_length=max_seq_len,weights=[embedd_matrix],trainable=False)\n",
    "gru_output_size=128\n",
    "bidirectional=True\n",
    "model=Sequential()\n",
    "model.add(embedd_layer)\n",
    "model.add(Bidirectional(GRU(units=gru_output_size,dropout=0.2,recurrent_dropout=0.2)))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9a3268",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8aefbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## train model\n",
    "\n",
    "batch_size=128\n",
    "epochs=8\n",
    "hist=model.fit(X_train_pad,y_train,batch_size=batch_size,epochs=epochs,validation_data=(X_test_pad,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d0b062",
   "metadata": {},
   "outputs": [],
   "source": [
    "## save model\n",
    "\n",
    "#model.save(\"text_emotion_classifier.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282c3c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "## lets visualize the function\n",
    "\n",
    "plt.figure(figsize=(10, 8), dpi = 150)\n",
    "loss_train = hist.history['loss']\n",
    "loss_val = hist.history['val_loss']\n",
    "epochs = range(1,9)\n",
    "plt.plot(epochs, loss_train, 'g', label='Training loss')\n",
    "plt.plot(epochs, loss_val, 'b', label='validation loss')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ee84ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "## lets visualize the function\n",
    "\n",
    "plt.figure(figsize=(10, 8), dpi = 150)\n",
    "loss_train = hist.history['accuracy']\n",
    "loss_val = hist.history['val_accuracy']\n",
    "epochs = range(1,9)\n",
    "plt.plot(epochs, loss_train, 'g', label='Training loss')\n",
    "plt.plot(epochs, loss_val, 'b', label='validation loss')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f699c5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## simple text deployment \n",
    "\n",
    "message=['I am sad.']\n",
    "seq=tokenizer.texts_to_sequences(message)\n",
    "padded=pad_sequences(seq,maxlen=max_seq_len)\n",
    "pred=model.predict(padded)\n",
    "print('Message:'+str(message))\n",
    "print('Emotion:',class_names[np.argmax(pred)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbc0759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95dfd558",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = load_model('text_emotion_classifier.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc8b89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Gradio app\n",
    "\n",
    "# def greet(massage):\n",
    "#     seq=tokenizer.texts_to_sequences(massage)\n",
    "#     padded=pad_sequences(seq,maxlen=max_seq_len)\n",
    "#     pred=model.predict(padded)\n",
    "#     result = class_names[np.argmax(pred)]\n",
    "#     return result\n",
    "\n",
    "\n",
    "# iface = gr.Interface(fn=greet, inputs=\"text\", outputs=\"text\")\n",
    "# iface.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
